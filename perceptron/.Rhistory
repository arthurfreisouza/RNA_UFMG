erro_grad <- 0
xseq <- sample(N) # Embaralhando as linhas do meu conjunto para melhorar o treino do meu modelo.
for (i in 1:N){
i_aleatorio <- xseq[i] # Pegando toda hora uma linha aleatória do meu dataset para treinar meu modelo.
xvec <- as.matrix(xin[i_aleatorio,]) # Pegando a entrada aleatória e a transformando em matriz.
yhat <- 1.0*((t(xvec) %*% wt) >= 0) # Calculando a saída do meu modelo yhat = (x^t).w
erro_actual <- (yd[i_aleatorio, ] - yhat) # Calculando o erro de saída.
dw <- (eta*erro_actual*xin[i_aleatorio, ]) # Atualizando parte da regra delta, no fito de ajustar corretamente os parametros.
wt <- wt + dw # Ajustando os parametros da rede.
erro_grad <- erro_grad + (erro_actual*erro_actual) # Acumulando o erro do gradiente em 1 variável.
}
nepocas = nepocas + 1
evec[nepocas] = erro_grad / N # Esse vetor conterá N médias de erros do gradiente, 1 para cada entrada.
erro_epoca <- evec[nepocas] # Para controlar o loop while.
}
retlist <- list(wt, evec[1 : nepocas]) # Retornando os pesos e os valores dos erros.
return(retlist)
}
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
xall <- rbind(xc1, xc2)
yall <- rbind(matrix(0, ncol = 1, nrow = nrow(xc1)), matrix(1, ncol = 1, nrow = nrow(xc2)))
retlist <- train_perceptron(xall, yall, 0.01, 0.1, 10000, 1)
w <- retlist[[1]]
w
xall_plus <- cbind(xall, 1)
yhat <- 1*((xall_plus %*% w) > 0)
et <- sum(t(yall - yhat) %*% (yall - yhat))
print(et)
x_values <- seq(0, 6, 0.1)
y_values <- - (w[1] * x_values + w[3]) / w[2]
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
par(new = TRUE)
lines(x_values, y_values, col = 'orange')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- 1*((xin %*% w) >= 0)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
library("mlbench")
library('plot3D')
# Limpando o ambiente.
rm(list = ls())
# Função para treinar o modelo perceptron
train_perceptron <- function(xin, yd, eta, tol, maxepocas, par) {
dimxin <- dim(xin) # Retorna o número de linhas e número de colunas da matriz de entrada xin.
N <- dimxin [1] # Pegando o número de linhas.(dados que gerarão 1 saída do modelo.)
n <- dimxin[2] # Pegando o número de colunas.(dados de entrada para cada epoch.)
if(par == 1){ # Par é um sinal de controle para um threshold.
wt <- as.matrix(runif(n+1) - 0.5) # Runif gerará valores aleatórios para inicializar a matriz de pesos.
xin <- cbind(xin, 1) # Criando a coluna de 1's porque terá 1 threshold de 0.5
}else{ # Caso não tenha um sinal de threshold.
wt <- as.matrix(runif(n) - 0.5)
}
nepocas <- 0 # O número de epocas que ja percorri sobre o conjunto de dados.
erro_epoca <- tol + 1 # A ideia é ir abaixando o erro_epoca.
evec <- matrix(nrow = 1, ncol = maxepocas)
while ((nepocas < maxepocas) && (erro_epoca > tol)){ # Loop para executar o treinamento.
erro_grad <- 0
xseq <- sample(N) # Embaralhando as linhas do meu conjunto para melhorar o treino do meu modelo.
for (i in 1:N){
i_aleatorio <- xseq[i] # Pegando toda hora uma linha aleatória do meu dataset para treinar meu modelo.
xvec <- as.matrix(xin[i_aleatorio,]) # Pegando a entrada aleatória e a transformando em matriz.
yhat <- 1.0*((t(xvec) %*% wt) >= 0) # Calculando a saída do meu modelo yhat = (x^t).w
erro_actual <- (yd[i_aleatorio, ] - yhat) # Calculando o erro de saída.
dw <- (eta*erro_actual*xin[i_aleatorio, ]) # Atualizando parte da regra delta, no fito de ajustar corretamente os parametros.
wt <- wt + dw # Ajustando os parametros da rede.
erro_grad <- erro_grad + (erro_actual*erro_actual) # Acumulando o erro do gradiente em 1 variável.
}
nepocas = nepocas + 1
evec[nepocas] = erro_grad / N # Esse vetor conterá N médias de erros do gradiente, 1 para cada entrada.
erro_epoca <- evec[nepocas] # Para controlar o loop while.
}
retlist <- list(wt, evec[1 : nepocas]) # Retornando os pesos e os valores dos erros.
return(retlist)
}
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
xall <- rbind(xc1, xc2)
yall <- rbind(matrix(0, ncol = 1, nrow = nrow(xc1)), matrix(1, ncol = 1, nrow = nrow(xc2)))
retlist <- train_perceptron(xall, yall, 0.1, 0.1, 10000, 1)
w <- retlist[[1]]
w
xall_plus <- cbind(xall, 1)
yhat <- 1*((xall_plus %*% w) > 0)
et <- sum(t(yall - yhat) %*% (yall - yhat))
print(et)
x_values <- seq(0, 6, 0.1)
y_values <- - (w[1] * x_values + w[3]) / w[2]
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
par(new = TRUE)
lines(x_values, y_values, col = 'orange')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- 1*((xin %*% w) >= 0)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
View(w)
library("mlbench")
library('plot3D')
# Limpando o ambiente.
rm(list = ls())
# Função para treinar o modelo perceptron
train_perceptron <- function(xin, yd, eta, tol, maxepocas, par) {
dimxin <- dim(xin) # Retorna o número de linhas e número de colunas da matriz de entrada xin.
N <- dimxin [1] # Pegando o número de linhas.(dados que gerarão 1 saída do modelo.)
n <- dimxin[2] # Pegando o número de colunas.(dados de entrada para cada epoch.)
if(par == 1){ # Par é um sinal de controle para um threshold.
wt <- as.matrix(runif(n+1) - 0.5) # Runif gerará valores aleatórios para inicializar a matriz de pesos.
xin <- cbind(xin, -1) # Criando a coluna de 1's porque terá 1 threshold de 0.5
}else{ # Caso não tenha um sinal de threshold.
wt <- as.matrix(runif(n) - 0.5)
}
nepocas <- 0 # O número de epocas que ja percorri sobre o conjunto de dados.
erro_epoca <- tol + 1 # A ideia é ir abaixando o erro_epoca.
evec <- matrix(nrow = 1, ncol = maxepocas)
while ((nepocas < maxepocas) && (erro_epoca > tol)){ # Loop para executar o treinamento.
erro_grad <- 0
xseq <- sample(N) # Embaralhando as linhas do meu conjunto para melhorar o treino do meu modelo.
for (i in 1:N){
i_aleatorio <- xseq[i] # Pegando toda hora uma linha aleatória do meu dataset para treinar meu modelo.
xvec <- as.matrix(xin[i_aleatorio,]) # Pegando a entrada aleatória e a transformando em matriz.
yhat <- 1.0*((t(xvec) %*% wt) >= 0) # Calculando a saída do meu modelo yhat = (x^t).w
erro_actual <- (yd[i_aleatorio, ] - yhat) # Calculando o erro de saída.
dw <- (eta*erro_actual*xin[i_aleatorio, ]) # Atualizando parte da regra delta, no fito de ajustar corretamente os parametros.
wt <- wt + dw # Ajustando os parametros da rede.
erro_grad <- erro_grad + (erro_actual*erro_actual) # Acumulando o erro do gradiente em 1 variável.
}
nepocas = nepocas + 1
evec[nepocas] = erro_grad / N # Esse vetor conterá N médias de erros do gradiente, 1 para cada entrada.
erro_epoca <- evec[nepocas] # Para controlar o loop while.
}
retlist <- list(wt, evec[1 : nepocas]) # Retornando os pesos e os valores dos erros.
return(retlist)
}
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
xall <- rbind(xc1, xc2)
yall <- rbind(matrix(0, ncol = 1, nrow = nrow(xc1)), matrix(1, ncol = 1, nrow = nrow(xc2)))
retlist <- train_perceptron(xall, yall, 0.01, 0.1, 10000, 1)
w <- retlist[[1]]
w
xall_plus <- cbind(xall, -1)
yhat <- 1*((xall_plus %*% w) > 0)
et <- sum(t(yall - yhat) %*% (yall - yhat))
print(et)
x_values <- seq(0, 6, 0.1)
y_values <- - (w[1] * x_values - w[3]) / w[2]
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
par(new = TRUE)
lines(x_values, y_values, col = 'orange')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- 1*((xin %*% w) >= 0)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
library("mlbench")
library('plot3D')
# Limpando o ambiente.
rm(list = ls())
# Função para treinar o modelo perceptron
train_perceptron <- function(xin, yd, eta, tol, maxepocas, par) {
dimxin <- dim(xin) # Retorna o número de linhas e número de colunas da matriz de entrada xin.
N <- dimxin [1] # Pegando o número de linhas.(dados que gerarão 1 saída do modelo.)
n <- dimxin[2] # Pegando o número de colunas.(dados de entrada para cada epoch.)
if(par == 1){ # Par é um sinal de controle para um threshold.
wt <- as.matrix(runif(n+1) - 0.5) # Runif gerará valores aleatórios para inicializar a matriz de pesos.
xin <- cbind(xin, -1) # Criando a coluna de 1's porque terá 1 threshold de 0.5
}else{ # Caso não tenha um sinal de threshold.
wt <- as.matrix(runif(n) - 0.5)
}
nepocas <- 0 # O número de epocas que ja percorri sobre o conjunto de dados.
erro_epoca <- tol + 1 # A ideia é ir abaixando o erro_epoca.
evec <- matrix(nrow = 1, ncol = maxepocas)
while ((nepocas < maxepocas) && (erro_epoca > tol)){ # Loop para executar o treinamento.
erro_grad <- 0
xseq <- sample(N) # Embaralhando as linhas do meu conjunto para melhorar o treino do meu modelo.
for (i in 1:N){
i_aleatorio <- xseq[i] # Pegando toda hora uma linha aleatória do meu dataset para treinar meu modelo.
xvec <- as.matrix(xin[i_aleatorio,]) # Pegando a entrada aleatória e a transformando em matriz.
yhat <- 1.0*((t(xvec) %*% wt) >= 0) # Calculando a saída do meu modelo yhat = (x^t).w
erro_actual <- (yd[i_aleatorio, ] - yhat) # Calculando o erro de saída.
dw <- (eta*erro_actual*xin[i_aleatorio, ]) # Atualizando parte da regra delta, no fito de ajustar corretamente os parametros.
wt <- wt + dw # Ajustando os parametros da rede.
erro_grad <- erro_grad + (erro_actual*erro_actual) # Acumulando o erro do gradiente em 1 variável.
}
nepocas = nepocas + 1
evec[nepocas] = erro_grad / N # Esse vetor conterá N médias de erros do gradiente, 1 para cada entrada.
erro_epoca <- evec[nepocas] # Para controlar o loop while.
}
retlist <- list(wt, evec[1 : nepocas]) # Retornando os pesos e os valores dos erros.
return(retlist)
}
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
xall <- rbind(xc1, xc2)
yall <- rbind(matrix(0, ncol = 1, nrow = nrow(xc1)), matrix(1, ncol = 1, nrow = nrow(xc2)))
retlist <- train_perceptron(xall, yall, 0.01, 0.1, 10000, 1)
w <- retlist[[1]]
w
xall_plus <- cbind(xall, -1)
yhat <- 1*((xall_plus %*% w) > 0)
et <- sum(t(yall - yhat) %*% (yall - yhat))
print(et)
x_values <- seq(0, 6, 0.1)
y_values <- - (w[1] * x_values - w[3]) / w[2]
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
par(new = TRUE)
lines(x_values, y_values, col = 'orange')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- 1*((xin %*% w) >= 0)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
View(yhat)
library("mlbench")
library('plot3D')
# Limpando o ambiente.
rm(list = ls())
# Função para treinar o modelo perceptron
train_perceptron <- function(xin, yd, eta, tol, maxepocas, par) {
dimxin <- dim(xin) # Retorna o número de linhas e número de colunas da matriz de entrada xin.
N <- dimxin [1] # Pegando o número de linhas.(dados que gerarão 1 saída do modelo.)
n <- dimxin[2] # Pegando o número de colunas.(dados de entrada para cada epoch.)
if(par == 1){ # Par é um sinal de controle para um threshold.
wt <- as.matrix(runif(n+1) - 0.5) # Runif gerará valores aleatórios para inicializar a matriz de pesos.
xin <- cbind(xin, -1) # Criando a coluna de 1's porque terá 1 threshold de 0.5
}else{ # Caso não tenha um sinal de threshold.
wt <- as.matrix(runif(n) - 0.5)
}
nepocas <- 0 # O número de epocas que ja percorri sobre o conjunto de dados.
erro_epoca <- tol + 1 # A ideia é ir abaixando o erro_epoca.
evec <- matrix(nrow = 1, ncol = maxepocas)
while ((nepocas < maxepocas) && (erro_epoca > tol)){ # Loop para executar o treinamento.
erro_grad <- 0
xseq <- sample(N) # Embaralhando as linhas do meu conjunto para melhorar o treino do meu modelo.
for (i in 1:N){
i_aleatorio <- xseq[i] # Pegando toda hora uma linha aleatória do meu dataset para treinar meu modelo.
xvec <- as.matrix(xin[i_aleatorio,]) # Pegando a entrada aleatória e a transformando em matriz.
yhat <- 1.0*((t(xvec) %*% wt) >= 0) # Calculando a saída do meu modelo yhat = (x^t).w
erro_actual <- (yd[i_aleatorio, ] - yhat) # Calculando o erro de saída.
dw <- (eta*erro_actual*xin[i_aleatorio, ]) # Atualizando parte da regra delta, no fito de ajustar corretamente os parametros.
wt <- wt + dw # Ajustando os parametros da rede.
erro_grad <- erro_grad + (erro_actual*erro_actual) # Acumulando o erro do gradiente em 1 variável.
}
nepocas = nepocas + 1
evec[nepocas] = erro_grad / N # Esse vetor conterá N médias de erros do gradiente, 1 para cada entrada.
erro_epoca <- evec[nepocas] # Para controlar o loop while.
}
retlist <- list(wt, evec[1 : nepocas]) # Retornando os pesos e os valores dos erros.
return(retlist)
}
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
xall <- rbind(xc1, xc2)
yall <- rbind(matrix(0, ncol = 1, nrow = nrow(xc1)), matrix(1, ncol = 1, nrow = nrow(xc2)))
retlist <- train_perceptron(xall, yall, 0.01, 0.1, 10000, 1)
w <- retlist[[1]]
w
xall_plus <- cbind(xall, -1)
yhat <- 1*((xall_plus %*% w) > 0)
et <- sum(t(yall - yhat) %*% (yall - yhat))
print(et)
x_values <- seq(0, 6, 0.1)
y_values <- - (w[1] * x_values - w[3]) / w[2]
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
par(new = TRUE)
lines(x_values, y_values, col = 'orange')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- 1*((xin %*% w) >= 0)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
library("mlbench")
library('plot3D')
# Limpando o ambiente.
rm(list = ls())
# Função para treinar o modelo perceptron
train_perceptron <- function(xin, yd, eta, tol, maxepocas, par) {
dimxin <- dim(xin) # Retorna o número de linhas e número de colunas da matriz de entrada xin.
N <- dimxin [1] # Pegando o número de linhas.(dados que gerarão 1 saída do modelo.)
n <- dimxin[2] # Pegando o número de colunas.(dados de entrada para cada epoch.)
if(par == 1){ # Par é um sinal de controle para um threshold.
wt <- as.matrix(runif(n+1) - 0.5) # Runif gerará valores aleatórios para inicializar a matriz de pesos.
xin <- cbind(xin, -1) # Criando a coluna de 1's porque terá 1 threshold de 0.5
}else{ # Caso não tenha um sinal de threshold.
wt <- as.matrix(runif(n) - 0.5)
}
nepocas <- 0 # O número de epocas que ja percorri sobre o conjunto de dados.
erro_epoca <- tol + 1 # A ideia é ir abaixando o erro_epoca.
evec <- matrix(nrow = 1, ncol = maxepocas)
while ((nepocas < maxepocas) && (erro_epoca > tol)){ # Loop para executar o treinamento.
erro_grad <- 0
xseq <- sample(N) # Embaralhando as linhas do meu conjunto para melhorar o treino do meu modelo.
for (i in 1:N){
i_aleatorio <- xseq[i] # Pegando toda hora uma linha aleatória do meu dataset para treinar meu modelo.
xvec <- as.matrix(xin[i_aleatorio,]) # Pegando a entrada aleatória e a transformando em matriz.
yhat <- 1.0*((t(xvec) %*% wt) >= 0) # Calculando a saída do meu modelo yhat = (x^t).w
erro_actual <- (yd[i_aleatorio, ] - yhat) # Calculando o erro de saída.
dw <- (eta*erro_actual*xin[i_aleatorio, ]) # Atualizando parte da regra delta, no fito de ajustar corretamente os parametros.
wt <- wt + dw # Ajustando os parametros da rede.
erro_grad <- erro_grad + (erro_actual*erro_actual) # Acumulando o erro do gradiente em 1 variável.
}
nepocas = nepocas + 1
evec[nepocas] = erro_grad / N # Esse vetor conterá N médias de erros do gradiente, 1 para cada entrada.
erro_epoca <- evec[nepocas] # Para controlar o loop while.
}
retlist <- list(wt, evec[1 : nepocas]) # Retornando os pesos e os valores dos erros.
return(retlist)
}
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
xall <- rbind(xc1, xc2)
yall <- rbind(matrix(0, ncol = 1, nrow = nrow(xc1)), matrix(1, ncol = 1, nrow = nrow(xc2)))
retlist <- train_perceptron(xall, yall, 0.01, 0.1, 10000, 1)
w <- retlist[[1]]
w
xall_plus <- cbind(xall, -1)
yhat <- 1*((xall_plus %*% w) > 0)
et <- sum(t(yall - yhat) %*% (yall - yhat))
print(et)
x_values <- seq(0, 6, 0.1)
y_values <- - (w[1] * x_values - w[3]) / w[2]
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
par(new = TRUE)
lines(x_values, y_values, col = 'orange')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- 1*((xin %*% w) >= 0)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
library("mlbench")
library('plot3D')
# Limpando o ambiente.
rm(list = ls())
# Função para treinar o modelo perceptron
train_perceptron <- function(xin, yd, eta, tol, maxepocas, par) {
dimxin <- dim(xin) # Retorna o número de linhas e número de colunas da matriz de entrada xin.
N <- dimxin [1] # Pegando o número de linhas.(dados que gerarão 1 saída do modelo.)
n <- dimxin[2] # Pegando o número de colunas.(dados de entrada para cada epoch.)
if(par == 1){ # Par é um sinal de controle para um threshold.
wt <- as.matrix(runif(n+1) - 0.5) # Runif gerará valores aleatórios para inicializar a matriz de pesos.
xin <- cbind(xin, -1) # Criando a coluna de 1's porque terá 1 threshold de 0.5
}else{ # Caso não tenha um sinal de threshold.
wt <- as.matrix(runif(n) - 0.5)
}
nepocas <- 0 # O número de epocas que ja percorri sobre o conjunto de dados.
erro_epoca <- tol + 1 # A ideia é ir abaixando o erro_epoca.
evec <- matrix(nrow = 1, ncol = maxepocas)
while ((nepocas < maxepocas) && (erro_epoca > tol)){ # Loop para executar o treinamento.
erro_grad <- 0
xseq <- sample(N) # Embaralhando as linhas do meu conjunto para melhorar o treino do meu modelo.
for (i in 1:N){
i_aleatorio <- xseq[i] # Pegando toda hora uma linha aleatória do meu dataset para treinar meu modelo.
xvec <- as.matrix(xin[i_aleatorio,]) # Pegando a entrada aleatória e a transformando em matriz.
yhat <- 1.0*((t(xvec) %*% wt) >= 0) # Calculando a saída do meu modelo yhat = (x^t).w
erro_actual <- (yd[i_aleatorio, ] - yhat) # Calculando o erro de saída.
dw <- (eta*erro_actual*xin[i_aleatorio, ]) # Atualizando parte da regra delta, no fito de ajustar corretamente os parametros.
wt <- wt + dw # Ajustando os parametros da rede.
erro_grad <- erro_grad + (erro_actual*erro_actual) # Acumulando o erro do gradiente em 1 variável.
}
nepocas = nepocas + 1
evec[nepocas] = erro_grad / N # Esse vetor conterá N médias de erros do gradiente, 1 para cada entrada.
erro_epoca <- evec[nepocas] # Para controlar o loop while.
}
retlist <- list(wt, evec[1 : nepocas]) # Retornando os pesos e os valores dos erros.
return(retlist)
}
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
xall <- rbind(xc1, xc2)
yall <- rbind(matrix(0, ncol = 1, nrow = nrow(xc1)), matrix(1, ncol = 1, nrow = nrow(xc2)))
retlist <- train_perceptron(xall, yall, 0.01, 0.1, 10000, 1)
w <- retlist[[1]]
w
xall_plus <- cbind(xall, -1)
yhat <- 1*((xall_plus %*% w) > 0)
et <- sum(t(yall - yhat) %*% (yall - yhat))
print(et)
x_values <- seq(0, 6, 0.1)
y_values <- - (w[1] * x_values - w[3]) / w[2]
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
par(new = TRUE)
lines(x_values, y_values, col = 'orange')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- 1*((xin %*% w) >= 0)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
rm(list = ls())
library('plot3D')
xc1 <- matrix(0.3*rnorm(60) + 2, ncol = 2)
xc2 <- matrix(0.3*rnorm(60) + 4, ncol = 2)
plot(xc1[, 1], xc1[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'red')
par(new = TRUE)
plot(xc2[, 1], xc2[, 2], xlim = c(0,6), ylim = c(0,6), xlab = 'x1', ylab = 'x2', col = 'blue')
seqx1x2 <- seq(0, 6, 0.2) # gera valores de 0 até 6 com passo de 0.2.
npgrid <- length(seqx1x2) # Pegando o tamanho da sequencia de valores gerados.
M <- matrix(nrow = npgrid, ncol = npgrid) # Criando 1 matriz quadrada.
ci <- 0
w <- as.matrix(c(1 ,1, 6)) # A matriz com os parâmetros
# Esses 2 valores de for alinhados gera os pares do plano (x1, x2) que irei plotar.
for (x1 in seqx1x2){
ci <- ci + 1
cj <- 0
for (x2 in seqx1x2){
cj <- cj + 1
xin <- as.matrix(cbind(x1, x2, -1))
M[ci, cj] <- tanh(xin %*% w)
}
}
ribbon3D(seqx1x2, seqx1x2, xlab = 'x1', ylab = 'x2', xlim = c(0, 6), ylim = c(0, 6), M, colkey =  F)
scatter3D(xc1[, 1], xc1[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'blue', colkey = F)
scatter3D(xc2[, 1], xc2[, 2], xlab = 'x1', ylab = 'x2', matrix(0, nrow = dim(xc1)[1]), add = T, col =  'red', colkey = F)
